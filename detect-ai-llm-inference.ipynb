{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6571530,"sourceType":"datasetVersion","datasetId":3796024},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":155261115,"sourceType":"kernelVersion"}],"dockerImageVersionId":30616,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U accelerate --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/","metadata":{"execution":{"iopub.status.busy":"2023-12-16T13:50:38.635946Z","iopub.execute_input":"2023-12-16T13:50:38.636267Z","iopub.status.idle":"2023-12-16T13:51:29.160740Z","shell.execute_reply.started":"2023-12-16T13:50:38.636243Z","shell.execute_reply":"2023-12-16T13:51:29.159617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport pandas as pd \nimport numpy as np\nfrom transformers import DataCollatorWithPadding\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom transformers import BitsAndBytesConfig\nimport peft","metadata":{"execution":{"iopub.status.busy":"2023-12-16T13:51:29.162629Z","iopub.execute_input":"2023-12-16T13:51:29.162894Z","iopub.status.idle":"2023-12-16T13:51:44.076797Z","shell.execute_reply.started":"2023-12-16T13:51:29.162872Z","shell.execute_reply":"2023-12-16T13:51:44.075845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassHead(nn.Module):\n    def __init__(self, input_dim, num_outputs):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, num_outputs)\n\n    def forward(self, x):\n        return self.linear(x)\n    \nclass Pooling(nn.Module):\n    def __init__(self, device):\n        super().__init__()\n        self.device = device\n        \n    def forward(self, x, attention_mask):\n        mean_pooled_batch = torch.zeros(x.shape[0], x.shape[2], device=self.device)\n        for i in range(attention_mask.shape[0]):\n            masked_embeddings = x[i] * attention_mask[i, :, None]\n            mean_pooled_batch[i] = torch.sum(masked_embeddings, axis=0) / torch.sum(attention_mask[i])\n        return mean_pooled_batch\n    \nclass ClassificationModel(nn.Module):\n    def __init__(self, backbone, device, num_outputs, quantization_config=None, lora_config=None):\n        super().__init__()\n        self.backbone = None\n        if quantization_config and lora_config:\n            if type(backbone) == str:\n                model = AutoModel.from_pretrained(backbone, quantization_config=quantization_config)\n            else: \n                model = backbone\n            self.backbone = get_peft_model(model, lora_config)           \n        else:\n            if type(backbone) == str:\n                self.backbone = AutoModel.from_pretrained(backbone)\n            else:\n                self.backbone = backbone\n        self.num_outputs = num_outputs\n        self.hidden_dim = self.backbone.config.hidden_size\n        self.device = device\n        self.pooling = Pooling(self.device)\n        self.head = ClassHead(self.hidden_dim, num_outputs)\n    \n    def forward(self, x, apply_softmax=False):\n        attention_mask = x['attention_mask']\n        x = self.backbone(**x)['last_hidden_state']\n        x = self.pooling(x, attention_mask)\n        x = self.head(x)\n        if self.num_outputs == 1:\n            x = x.view(-1)\n        if apply_softmax:\n            x = nn.functional.softmax(x, dim=1)\n        return x \n    \nclass CustomDataset(Dataset):\n    def __init__(self, backbone, path, max_length, text_col, sep=',', id_col=None, target_col=None, \n                 cat_col=None, sample=None, tokenizer=None) -> None:\n        super().__init__()\n        df = pd.read_csv(path, sep=sep)\n        if sample:\n            df = df.sample(sample)\n        self.tokenizer = None\n        if tokenizer:\n            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n        if not tokenizer:\n            self.tokenizer = AutoTokenizer.from_pretrained(backbone)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.collator = DataCollatorWithPadding(self.tokenizer, padding=True)\n        self.tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n        self.ids = df[id_col].values.tolist() if id_col in df.columns else None\n        self.texts = None\n        if text_col in df.columns:\n            if cat_col in df.columns:\n                df[text_col] = df[cat_col] + ' [SEP] ' + df[text_col]\n            self.texts = df[text_col].values.tolist()\n        self.labels = df[target_col].values.tolist() if target_col in df.columns else None\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        r = self.tokenizer(\n            text=self.texts[index],\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors='pt'\n        )\n        r = {k : v[0] for k, v in r.items()}\n        if self.labels:\n            r['labels'] = self.labels[index]\n        return r","metadata":{"execution":{"iopub.status.busy":"2023-12-16T13:51:44.078309Z","iopub.execute_input":"2023-12-16T13:51:44.079006Z","iopub.status.idle":"2023-12-16T13:51:44.099832Z","shell.execute_reply.started":"2023-12-16T13:51:44.078971Z","shell.execute_reply":"2023-12-16T13:51:44.098809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def class_inference_loop(model, dataloader, device, **predict_kwargs):\n    model.to(device)\n    predictions = []\n    labels = []\n    \n    model.eval()\n    with torch.no_grad(), torch.autocast(device_type=device):\n        for batch_dict in tqdm(dataloader):\n            y = None\n            if 'labels' in batch_dict:\n                y = batch_dict[\"labels\"].float().to(device)\n                del batch_dict['labels']\n            x = batch_dict\n            x = {k:v.to(device) for k, v in x.items()}\n            \n            logits = model(x, **predict_kwargs)\n            probs = nn.functional.sigmoid(logits)\n            predictions.extend(probs.cpu().tolist())\n            if y is not None:\n                labels.extend(y.cpu().tolist())  \n    return predictions, labels","metadata":{"execution":{"iopub.status.busy":"2023-12-16T13:51:44.102217Z","iopub.execute_input":"2023-12-16T13:51:44.102588Z","iopub.status.idle":"2023-12-16T13:51:44.384566Z","shell.execute_reply.started":"2023-12-16T13:51:44.102553Z","shell.execute_reply":"2023-12-16T13:51:44.383688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nbackbone = AutoModel.from_pretrained('/kaggle/input/detect-ai-llm-train/encoder_model')\nmodel = ClassificationModel(backbone, 'cuda', 1, None, None)\nmodel.head.load_state_dict(torch.load('/kaggle/input/detect-ai-llm-train/encoder_head.pth'))\n\nmodel.eval()\n\ntest_ds = CustomDataset('', '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv', 512, \n                        'text', ',', 'id', None, None, sample=None, tokenizer='/kaggle/input/detect-ai-llm-train/encoder_tokenizer')\ntest_dl = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=test_ds.collator)\npred, _ =  class_inference_loop(model, test_dl, 'cuda')\ndf = pd.DataFrame({'id': test_ds.ids, 'generated': pred})\ndf.to_csv('submission.csv', sep=',', index=False)\n'''\n\nquantization_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.float16,\n)\n\nmistral = AutoModel.from_pretrained(\"/kaggle/input/mistral-7b-v0-1/Mistral-7B-v0.1\", quantization_config=quantization_config)\nbackbone = peft.PeftModel.from_pretrained(\n    model=mistral,\n    model_id='/kaggle/input/detect-ai-llm-train/decoder_model',\n    is_trainable=False\n)\nmodel = ClassificationModel(backbone, 'cuda', 1, None, None)\nmodel.head.load_state_dict(torch.load('/kaggle/input/detect-ai-llm-train/decoder_head.pth'))\n\nmodel.eval()\n\ntest_ds = CustomDataset('', '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv', 64, \n                        'text', ',', 'id', None, None, sample=None, tokenizer='/kaggle/input/detect-ai-llm-train/decoder_tokenizer')\ntest_dl = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=test_ds.collator)\npred, _ =  class_inference_loop(model, test_dl, 'cuda')\ndf = pd.DataFrame({'id': test_ds.ids, 'generated': pred})\ndf.to_csv('submission.csv', sep=',', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-16T13:51:44.385692Z","iopub.execute_input":"2023-12-16T13:51:44.385950Z","iopub.status.idle":"2023-12-16T13:54:33.965488Z","shell.execute_reply.started":"2023-12-16T13:51:44.385928Z","shell.execute_reply":"2023-12-16T13:54:33.964580Z"},"trusted":true},"execution_count":null,"outputs":[]}]}