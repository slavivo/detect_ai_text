{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6412810,"sourceType":"datasetVersion","datasetId":3698412},{"sourceId":6865136,"sourceType":"datasetVersion","datasetId":3945154},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644},{"sourceId":6901341,"sourceType":"datasetVersion","datasetId":3960967},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":30588,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🛠️ Setup\n---\n\n## This notebook draws from the original paragraph by  ERTUĞRUL DEMIR\n- **Notebook**\n  - Kaggle URL: https://www.kaggle.com/code/datafan07/train-your-own-tokenizer/notebook","metadata":{}},{"cell_type":"code","source":"cd /kaggle/input/py-readability-metrics","metadata":{"execution":{"iopub.status.busy":"2023-12-17T14:49:43.180391Z","iopub.execute_input":"2023-12-17T14:49:43.181158Z","iopub.status.idle":"2023-12-17T14:49:43.187909Z","shell.execute_reply.started":"2023-12-17T14:49:43.181124Z","shell.execute_reply":"2023-12-17T14:49:43.186867Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/py-readability-metrics\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install py_readability_metrics-1.4.5-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-12-17T14:50:03.139414Z","iopub.execute_input":"2023-12-17T14:50:03.140290Z","iopub.status.idle":"2023-12-17T14:50:35.544568Z","shell.execute_reply.started":"2023-12-17T14:50:03.140260Z","shell.execute_reply":"2023-12-17T14:50:35.543444Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Processing ./py_readability_metrics-1.4.5-py3-none-any.whl\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from py-readability-metrics==1.4.5) (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->py-readability-metrics==1.4.5) (1.16.0)\nInstalling collected packages: py-readability-metrics\nSuccessfully installed py-readability-metrics-1.4.5\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-12-17T14:51:04.363202Z","iopub.execute_input":"2023-12-17T14:51:04.364130Z","iopub.status.idle":"2023-12-17T14:51:04.369858Z","shell.execute_reply.started":"2023-12-17T14:51:04.364091Z","shell.execute_reply":"2023-12-17T14:51:04.369021Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nimport gc\n\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom readability import Readability\nfrom scipy.sparse import csr_matrix, hstack","metadata":{"execution":{"iopub.status.busy":"2023-12-17T14:52:05.570028Z","iopub.execute_input":"2023-12-17T14:52:05.570406Z","iopub.status.idle":"2023-12-17T14:52:15.301024Z","shell.execute_reply.started":"2023-12-17T14:52:05.570375Z","shell.execute_reply":"2023-12-17T14:52:15.300034Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data preparation","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\norg_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\ntrain = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\n\ntrain = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)\n\ndef extract_scores(text):\n    try:\n        r = Readability(text)\n        scores =  [r.flesch_kincaid().score, r.flesch().score, r.gunning_fog().score, r.coleman_liau().score, r.dale_chall().score, r.ari().score, r.linsear_write().score, r.spache().score]\n        return [round(x, 3) + 1000 for x in scores]\n    except:\n        return [0]*8\n    \ntrain = train.sample(1000, random_state = 1)\n    \ntrain['scores'] = train['text'].map(extract_scores)\nscores_train = train['scores'].apply(pd.Series)\nscores_train.columns = [ f\"scores_{x}\" for x in scores_train.columns]\ngc.collect()\n\ntest['scores'] = test['text'].map(extract_scores)\nscores_test = test['scores'].apply(pd.Series)\nscores_test.columns = [ f\"scores_{x}\" for x in scores_test.columns]\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T14:53:59.558756Z","iopub.execute_input":"2023-12-17T14:53:59.559141Z","iopub.status.idle":"2023-12-17T15:15:03.262668Z","shell.execute_reply.started":"2023-12-17T14:53:59.559113Z","shell.execute_reply":"2023-12-17T15:15:03.261685Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"LOWERCASE = False\nVOCAB_SIZE = 30522\n\n\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\nraw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\ndataset = Dataset.from_pandas(test[['text']])\n\ndef train_corp_iter(): \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\n        \nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\ntokenized_texts_test = []\n\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text))\n\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T15:15:38.384202Z","iopub.execute_input":"2023-12-17T15:15:38.385065Z","iopub.status.idle":"2023-12-17T15:15:40.956725Z","shell.execute_reply.started":"2023-12-17T15:15:38.385031Z","shell.execute_reply":"2023-12-17T15:15:40.955774Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if _pandas_api.is_sparse(col):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e8416e9449a4ccaaf5692e8846653dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cbf593e681a40dfb4bbab288b743dba"}},"metadata":{}}]},{"cell_type":"markdown","source":"## TF-IDF","metadata":{}},{"cell_type":"code","source":"def dummy(text):\n    return text\n\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, \n                             analyzer = 'word', tokenizer = dummy,preprocessor = dummy,\n                             token_pattern = None, strip_accents='unicode')\n\nvectorizer.fit(tokenized_texts_test)\n\nvocab = vectorizer.vocabulary_\n\nprint(vocab)\n\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n                            analyzer = 'word', tokenizer = dummy, preprocessor = dummy,\n                            token_pattern = None, strip_accents='unicode')\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train)\ntf_test = vectorizer.transform(tokenized_texts_test)\n\ndel vectorizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T15:15:40.958251Z","iopub.execute_input":"2023-12-17T15:15:40.958547Z","iopub.status.idle":"2023-12-17T15:15:46.823032Z","shell.execute_reply.started":"2023-12-17T15:15:40.958523Z","shell.execute_reply":"2023-12-17T15:15:46.822180Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"{'ĠAaa Ġbbb Ġccc': 0, 'Ġbbb Ġccc .': 6, 'ĠAaa Ġbbb Ġccc .': 1, 'ĠBbb Ġccc Ġddd': 2, 'Ġccc Ġddd .': 7, 'ĠBbb Ġccc Ġddd .': 3, 'ĠCCC Ġddd Ġeee': 4, 'Ġddd Ġeee .': 8, 'ĠCCC Ġddd Ġeee .': 5}\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"44"},"metadata":{}}]},{"cell_type":"markdown","source":"## Classifier ensemble","metadata":{}},{"cell_type":"code","source":"df_sparse = csr_matrix(scores_train.values)\ntf_train = hstack([df_sparse, tf_train])\n\ndf_sparse = csr_matrix(scores_test.values)\ntf_test = hstack([df_sparse, tf_test])\ngc.collect()\n\ny_train = train['label'].values","metadata":{"execution":{"iopub.status.busy":"2023-12-17T15:15:46.824900Z","iopub.execute_input":"2023-12-17T15:15:46.825281Z","iopub.status.idle":"2023-12-17T15:15:46.967759Z","shell.execute_reply.started":"2023-12-17T15:15:46.825247Z","shell.execute_reply":"2023-12-17T15:15:46.966584Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_sparse \u001b[38;5;241m=\u001b[39m csr_matrix(scores_train\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 2\u001b[0m tf_train \u001b[38;5;241m=\u001b[39m \u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_sparse \u001b[38;5;241m=\u001b[39m csr_matrix(scores_test\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      5\u001b[0m tf_test \u001b[38;5;241m=\u001b[39m hstack([df_sparse, tf_test])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/sparse/construct.py:467\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhstack\u001b[39m(blocks, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    Stack sparse matrices horizontally (column wise)\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m \n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/sparse/construct.py:588\u001b[0m, in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m brow_lengths[i] \u001b[38;5;241m!=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    583\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks[\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m,:] has incompatible row dimensions. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    584\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGot blocks[\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{j}\u001b[39;00m\u001b[38;5;124m].shape[0] == \u001b[39m\u001b[38;5;132;01m{got}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    585\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected \u001b[39m\u001b[38;5;132;01m{exp}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m=\u001b[39mi, j\u001b[38;5;241m=\u001b[39mj,\n\u001b[1;32m    586\u001b[0m                                     exp\u001b[38;5;241m=\u001b[39mbrow_lengths[i],\n\u001b[1;32m    587\u001b[0m                                     got\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bcol_lengths[j] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    591\u001b[0m     bcol_lengths[j] \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n","\u001b[0;31mValueError\u001b[0m: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 1000, expected 44868."],"ename":"ValueError","evalue":"blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 1000, expected 44868.","output_type":"error"}]},{"cell_type":"code","source":"if len(test.text.values) <= 5:\n    sub.to_csv('submission.csv', index=False)\nelse:\n    clf = MultinomialNB(alpha=0.02)\n    sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n    p6={'n_iter': 1500,'verbose': -1,'objective': 'cross_entropy','metric': 'auc',\n        'learning_rate': 0.05073909898961407, 'colsample_bytree': 0.726023996436955,\n        'colsample_bynode': 0.5803681307354022, 'lambda_l1': 8.562963348932286, \n        'lambda_l2': 4.893256185259296, 'min_data_in_leaf': 115, 'max_depth': 23, 'max_bin': 898}\n    lgb=LGBMClassifier(**p6)\n    cat=CatBoostClassifier(iterations=1000,\n                           verbose=0,\n                           l2_leaf_reg=6.6591278779517808,\n                           learning_rate=0.005689066836106983,\n                           allow_const_label=True,loss_function = 'CrossEntropy')\n    weights = [0.07,0.31,0.31,0.31]\n \n    ensemble = VotingClassifier(estimators=[('mnb',clf),\n                                            ('sgd', sgd_model),\n                                            ('lgb',lgb), \n                                            ('cat', cat)\n                                           ],\n                                weights=weights, voting='soft', n_jobs=-1)\n    ensemble.fit(tf_train, y_train)\n    gc.collect()\n    final_preds = ensemble.predict_proba(tf_test)[:,1]\n    sub['generated'] = final_preds\n    sub.to_csv('submission.csv', index=False)\n    sub","metadata":{"execution":{"iopub.status.busy":"2023-12-17T15:15:46.968441Z","iopub.status.idle":"2023-12-17T15:15:46.968792Z","shell.execute_reply.started":"2023-12-17T15:15:46.968619Z","shell.execute_reply":"2023-12-17T15:15:46.968635Z"},"trusted":true},"execution_count":null,"outputs":[]}]}